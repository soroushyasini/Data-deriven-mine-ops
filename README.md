# Gold Mining Operations Data Platform

A comprehensive data-driven decision-making platform for gold mining supply chain operations in Iran. This system automates data ingestion, validation, analysis, and reporting for the complete mining â†’ grinding â†’ processing â†’ lab analysis workflow.

## ğŸŒŸ Features

- **Automated Data Pipeline**: Convert Excel/JSON data from trucking, grinding facilities, and lab analysis
- **Persian/Farsi Support**: Full UTF-8 support with Persian text handling and Jalali calendar dates
- **Data Validation**: Real-time alerts for anomalies, quality issues, and critical thresholds
- **Complete Tracing**: Link lab results back through bunker loads to original mine shipments
- **Multi-Channel Alerts**: Telegram, Email, Dashboard, and persistent logs
- **Professional Reports**: PDF and Markdown reports for management and technical teams
- **Interactive Dashboard**: Metabase-powered analytics and visualization
- **Docker Deployment**: One-command deployment with docker-compose

## ğŸ“‹ Supply Chain Flow

```
Mine (Single Source)
  â†“ [Trucking]
Grinding Facilities (3)
  â€¢ Hejazian (A) - Ø±Ø¨Ø§Ø· Ø³ÙÛŒØ¯
  â€¢ Shen Beton (B) - Ø´Ù† Ø¨ØªÙ† Ù…Ø´Ù‡Ø¯
  â€¢ Kavian (C) - Ø´Ù‡Ø±Ú© Ú©Ø§ÙˆÛŒØ§Ù†
  â†“ [Bunker Transport]
Processing Factory
  â†“ [Sampling]
Lab Analysis
  â†“ [Results]
Decision Feedback
```

## ğŸš€ Quick Start

### Prerequisites

- Docker and Docker Compose
- Git

### Installation

1. **Clone the repository**:
   ```bash
   git clone https://github.com/soroushyasini/Data-deriven-mine-ops.git
   cd Data-deriven-mine-ops
   ```

2. **Create environment file** (optional for alerts):
   ```bash
   cp .env.example .env
   # Edit .env with your credentials
   ```

3. **Start the system**:
   ```bash
   docker-compose up -d
   ```

4. **Initialize the database**:
   ```bash
   docker-compose exec pipeline python scripts/init_db.py
   ```

5. **Access the dashboard**:
   - Open http://localhost:3000 in your browser
   - Default Metabase setup wizard will guide you

### Data Ingestion

1. **Place data files** in `data/incoming/`:
   - `trucking_data_for_llm.json` - Mine to grinding truck shipments
   - `data_for_llm_enhanced.json` - Grinding to factory bunker loads
   - `lab_data_for_llm.json` - Lab analysis results

2. **Run ingestion pipeline**:
   ```bash
   docker-compose exec pipeline python scripts/ingest.py
   ```

3. **Generate reports**:
   ```bash
   # Daily operations report
   docker-compose exec pipeline python scripts/generate_report.py daily --date 1404/10/14
   
   # Grade report for specific facility
   docker-compose exec pipeline python scripts/generate_report.py grade --facility A
   
   # All reports
   docker-compose exec pipeline python scripts/generate_report.py all
   ```

## ğŸ“ Project Structure

```
Data-deriven-mine-ops/
â”œâ”€â”€ config/                      # Configuration files (JSON)
â”‚   â”œâ”€â”€ facilities.json         # Facility registry (A/B/C)
â”‚   â”œâ”€â”€ drivers.json            # Driver canonical names & aliases
â”‚   â”œâ”€â”€ trucks.json             # Truck registry
â”‚   â”œâ”€â”€ sample_types.json       # Lab sample type definitions
â”‚   â””â”€â”€ validation_rules.json   # Alert thresholds
â”‚
â”œâ”€â”€ src/                        # Source code
â”‚   â”œâ”€â”€ core/                   # Core utilities
â”‚   â”‚   â”œâ”€â”€ base_converter.py  # Persian text handling, date normalization
â”‚   â”‚   â”œâ”€â”€ validator.py       # Data validation engine
â”‚   â”‚   â””â”€â”€ linker.py          # Lab â†’ Bunker â†’ Truck tracing
â”‚   â”‚
â”‚   â”œâ”€â”€ converters/             # Data converters
â”‚   â”‚   â”œâ”€â”€ bunker_converter.py
â”‚   â”‚   â”œâ”€â”€ assay_converter.py
â”‚   â”‚   â”œâ”€â”€ trucking_converter.py
â”‚   â”‚   â””â”€â”€ finance_converter.py
â”‚   â”‚
â”‚   â”œâ”€â”€ database/               # Database layer
â”‚   â”‚   â”œâ”€â”€ models.py          # SQLAlchemy ORM models
â”‚   â”‚   â”œâ”€â”€ connection.py      # Database connection
â”‚   â”‚   â””â”€â”€ ingestion.py       # Data loading
â”‚   â”‚
â”‚   â”œâ”€â”€ reports/                # Report generators
â”‚   â”‚   â”œâ”€â”€ base_report.py     # Base class (PDF + Markdown)
â”‚   â”‚   â”œâ”€â”€ daily_ops.py       # Daily operations
â”‚   â”‚   â””â”€â”€ grade_report.py    # Grade analysis
â”‚   â”‚
â”‚   â””â”€â”€ alerts/                 # Alert system
â”‚       â”œâ”€â”€ alert_engine.py    # Core alert logic
â”‚       â”œâ”€â”€ telegram_notifier.py
â”‚       â”œâ”€â”€ email_notifier.py
â”‚       â””â”€â”€ log_notifier.py
â”‚
â”œâ”€â”€ scripts/                    # Utility scripts
â”‚   â”œâ”€â”€ init_db.py             # Initialize database
â”‚   â”œâ”€â”€ ingest.py              # Run ingestion pipeline
â”‚   â””â”€â”€ generate_report.py     # Generate reports
â”‚
â”œâ”€â”€ data/                       # Data directories
â”‚   â”œâ”€â”€ incoming/              # Drop files here
â”‚   â”œâ”€â”€ processed/             # Converted JSON output
â”‚   â””â”€â”€ samples/               # Sample data
â”‚
â”œâ”€â”€ docker-compose.yml          # Docker orchestration
â”œâ”€â”€ Dockerfile                  # Python container
â”œâ”€â”€ requirements.txt            # Python dependencies
â””â”€â”€ README.md                   # This file
```

## ğŸ”§ Configuration

### Facilities (config/facilities.json)

Defines the three grinding facilities with Persian and English names:

```json
{
  "A": {
    "name_fa": "Ø­Ø¬Ø§Ø²ÛŒØ§Ù†",
    "name_en": "Hejazian",
    "bunker_sheet": "Ø±Ø¨Ø§Ø· Ø³ÙÛŒØ¯",
    "truck_dest": "Ø±Ø¨Ø§Ø· Ø³ÙÛŒØ¯"
  }
}
```

### Drivers (config/drivers.json)

Canonical driver names with spelling variants:

```json
{
  "canonical_drivers": {
    "Ù…Ø­Ù…Ø¯ Ø§Ø­Ù…Ø¯Ø¢Ø¨Ø§Ø¯ÛŒ": {
      "aliases": ["Ù…Ø­Ù…Ø¯ Ø§Ø­Ù…Ø¯ Ø¢Ø¨Ø§Ø¯ÛŒ", "Ù…Ø­Ù…Ø¯ Ø§Ø­Ù…Ø¯Ø¢Ø¨Ø§Ø¯ÛŒ"],
      "status": "active"
    }
  }
}
```

### Validation Rules (config/validation_rules.json)

Alert thresholds for different sample types and conditions:

```json
{
  "ore_input": {
    "warning_threshold_ppm": 5.0,
    "critical_threshold_ppm": 20.0
  },
  "tailings": {
    "critical_threshold_ppm": 0.2
  }
}
```

## ğŸ“Š Sample Code Format

Lab samples use a standardized code format that enables complete tracing:

```
C 1404 10 14 K2
â”‚  â”‚    â”‚  â”‚  â”‚
â”‚  â”‚    â”‚  â”‚  â””â”€ Sample type + number
â”‚  â”‚    â”‚  â”‚      K = Ore Input
â”‚  â”‚    â”‚  â”‚      L = Solution
â”‚  â”‚    â”‚  â”‚      T = Tailings
â”‚  â”‚    â”‚  â”‚      CR = Carbon
â”‚  â”‚    â”‚  â””â”€â”€â”€â”€ Day of month
â”‚  â”‚    â””â”€â”€â”€â”€â”€â”€â”€ Month (Jalali)
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Year (Jalali, 1404)
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€ Facility (A/B/C)
```

## ğŸš¨ Alert Rules

The system monitors for:

- **Ore Input (K)**: Au > 5 ppm (warning), Au > 20 ppm (critical)
- **Tailings (T)**: Au > 0.2 ppm (critical - gold loss)
- **Return Water (RC)**: Au > 0.05 ppm (critical - circuit leak)
- **Carbon (CR)**: Au < 200 ppm (warning - exhausted)
- **Tonnage**: < 15,000 or > 32,000 kg (unusual load)
- **Data Quality**: Missing receipts, unknown drivers, invalid sample codes

## ğŸ“ˆ Reports

### Daily Operations Report
- Truck shipments
- Bunker loads received
- Lab samples processed
- By-facility breakdown

### Grade Report per Facility
- Au ppm statistics by sample type
- Trend analysis
- Outlier detection
- Process recommendations

### Additional Reports (Planned)
- Facility comparison
- Finance summary
- Monthly executive summary

## ğŸ”” Alert Channels

1. **Telegram Bot**: Real-time alerts to team chat
2. **Email**: Critical alerts and daily digest
3. **Dashboard**: Alert panel in Metabase
4. **Log File**: Persistent record in `logs/alerts.log`

### Setting Up Alerts

Edit `.env` file with your credentials:

```bash
# Telegram
TELEGRAM_BOT_TOKEN=your_bot_token
TELEGRAM_CHAT_ID=your_chat_id

# Email
SMTP_HOST=smtp.gmail.com
SMTP_PORT=587
SMTP_USERNAME=your_email@gmail.com
SMTP_PASSWORD=your_app_password
EMAIL_FROM=alerts@mining-ops.com
EMAIL_TO=manager@mining-ops.com,engineer@mining-ops.com
```

## ğŸ—„ï¸ Database Schema

PostgreSQL database with the following tables:

- `facilities` - Grinding facility registry
- `drivers` - Driver canonical names and aliases
- `trucks` - Truck registry
- `shipments` - Mine â†’ Grinding truck shipments
- `bunker_loads` - Grinding â†’ Factory bunker loads
- `lab_samples` - Lab analysis results
- `transport_costs` - Historical transport cost records
- `payments` - Driver payment tracking
- `alerts` - Alert history

## ğŸ§ª Development

### Running Tests

```bash
pytest tests/
```

### Adding New Data Sources

1. Create converter in `src/converters/`
2. Extend database models in `src/database/models.py`
3. Update ingestion script `scripts/ingest.py`
4. Add configuration if needed

### Adding New Reports

1. Create report class in `src/reports/`
2. Inherit from `BaseReport`
3. Implement `generate_data()` and `format_markdown()`
4. Add to `scripts/generate_report.py`

## ğŸ“ Data Quality Features

- **Persian number cleaning**: Remove commas, convert `/` to `.`
- **Date normalization**: Jalali dates to YYYY/MM/DD format
- **Driver name canonicalization**: Handle spelling variants
- **Truck number cleaning**: Remove `.0` suffix from floats
- **Cost calculation fixes**: Proper ton-based calculations
- **Null row filtering**: Automatic removal of empty rows
- **Column typo correction**: Fix known Persian typos

## ğŸŒ Persian/Farsi Support

All text handling uses UTF-8 encoding. The system natively supports:

- Persian column names in Excel files
- Jalali (Persian) calendar dates
- Persian driver and facility names
- Bi-directional text in reports

## ğŸ’° Currency

The system uses Iranian Rial as the base currency:
- 1 Toman = 10 Rial
- Reports display both Rial and Toman where relevant

## ğŸ¤ Contributing

This is a specialized platform for a specific operation. For questions or issues, please open a GitHub issue.

## ğŸ“„ License

MIT License - See LICENSE file for details

## ğŸ™ Acknowledgments

Built for gold mining operations in Iran, automating decision-making and replacing WhatsApp-based communication with a proper data pipeline.

## ğŸ“ Support

For questions or support:
- Open an issue on GitHub
- Review the documentation in `/docs` (coming soon)
- Check the example data in `/data/samples`

## ğŸš¦ Status

âœ… Core data pipeline complete
âœ… Database schema and ingestion
âœ… Alert system with multiple channels
âœ… Report generation (PDF + Markdown)
âœ… Docker deployment
ğŸš§ Advanced analytics (in progress)
ğŸš§ Predictive modeling (planned)
ğŸš§ Mobile app (planned)
